---
title: "Milestone"
output: 
  html_document: 
    keep_md: yes
---

```{r options, echo = FALSE}
library(knitr)
opts_chunk$set(cache = TRUE, echo = FALSE, comment = NA)
```

##Corpora Summary  
```{r read in EN}
en <- readRDS("data/readLines_list_all_EN.RDS")
```

Before breaking down any text into *n*-grams or doing any natural language processing, we'll 
explore the texts just as they are provided to us. We have three corpora, namely: twitter; news; and
blogs. Here we see the number of texts in each corpus, along with the mean, median, and max 
characters per text.  

```{r corpora summary}
chars <- sapply(en, nchar)
summ <- data.frame("Texts" = sapply(en, length), "Average" = sapply(chars, mean), 
           "Median" = sapply(chars, median), "Maximum" = sapply(chars, max))
round(summ, 0)

```

The average length of news and blogs are a bit higher than the medians. This skew is caused by some 
very long texts. The longest text in our corpora ia a blog, with more than 40 thousand characters. 
Note the 140 character limit on tweets.  

##Tokenizing  
The first step in building a language model is to **tokenize**, which involves break a sentence 
down into individual words, or pairs of words, or triplets of words, etc. These tokens are what we 
call *n*-grams, where *n* is the number of words. 

At this point we also make the decision to filter out whitespace and puncutation, and covert all 
text to lowercase. This makes for easier processing down the line, by having a more uniform set of 
texts. 

##Model Design  
To save space, we will not store all possible *n*-grams indexed by their *n-1*-grams. Instead, we 
only need to store the *n*-gram with the highest frequency of it's *n-1*-gram index. For instance, 
if we have a term frequency list as follows:  

Term    Frequency    index
------- ---------- --------
the box  .05        the  
the cat  .15        the  
the oven  .02       the  
the king  .01       the  

We will only store the highest frequency term for the given index, "the". In fact, we don't even
need the frequency at that point.  

Term     index
-------  --------
the box  the  


##Filtering  

Filtering, such as profanity and numbers, are done at the n-gram level, so we don't have sentences
with gaps


##Future Plans  
Plan to add a part of speech (POS) tagger for unseen n-grams. We'll create a small frequency list 
for each of the parts of speech (possibly trained on our data). Then instead of blindly returning 
the highest occuring token, we will return the highest occuring token given the POS.  

We will keep a small number of lines from each corpus as a test set. We can create lots of *n*-gram
samples from just a few lines to test our model.  

Don't use backoff only for grams that don't appear. IE Where is the n-1 gram more frequent? From 
that take the nth word for the model.  