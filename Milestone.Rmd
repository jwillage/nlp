---
title: "Milestone"
output: 
  html_document: 
    keep_md: yes
---

```{r options, echo = FALSE}
library(knitr)
library(quanteda)
library(ggplot2)
library(gridExtra)
opts_chunk$set(cache = TRUE, echo = FALSE, comment = NA, message = FALSE, warnings = FALSE)
```

##Exploratory Analysis

###Corpora Summary  
```{r read in EN}
en <- readRDS("data/readLines_list_all_EN.RDS")
```

Before breaking down any text and performing natural language processing, we'll 
explore the texts just as they are provided to us. We have three corpora, namely: twitter; news; and
blogs. Here we see the number of texts in each corpus, along with the mean, median, and max 
characters per text.  

```{r corpora summary}
chars <- sapply(en, nchar)
summ <- data.frame("Texts" = sapply(en, length), "Average" = sapply(chars, mean), 
           "Median" = sapply(chars, median), "Maximum" = sapply(chars, max))
round(summ, 0)

```

The average length of news and blogs are a bit higher than the medians. This skew is caused by some 
very long texts. The longest text in our corpora is a blog with more than 40 thousand characters. 
Note the 140 character limit on tweets.  

###Tokenizing  
The first step in building a language model is **tokenizing**, which involves break a sentence 
down into individual words (or pairs of words, or triplets of words, etc). These tokens are what we 
call *n*-grams, where *n* is the number of words. For example, "the", "cat", "jumped" are all 
1-grams (uni-grams), whereas "the cat", "cat jumped" are 2-grams (bigrams).

During the tokenization phase, we also make the decision to filter out whitespace and puncutation,
and covert all text to lowercase. This makes for easier processing down the line, by having a more
uniform set of texts. 

```{r tokenize and dfm}
corp <- lapply(en, corpus)
rm(en)

tok.list <- lapply(corp, function(x) toLower(tokenize(x, removeNumbers = F, removePunct = T, 
                                                      removeSeparators = T)))
#create document frequency matrices
dfm.en <- lapply(tok.list, dfm)
```

Let's take a look at the most frequent tokens of uni-grams and their counts.  

```{r topfeatures}
tf <-lapply(dfm.en, function(x) {
  t <- topfeatures(x, 15)
  data.frame(frequency = t, term = names(t))})

g <- lapply(tf, function(x) ggplot(aes(x = reorder(term, frequency, min), y = frequency), data = x)
            + geom_histogram(stat = "identity") + xlab("term"))
for (i in 1:length(g)) g[[i]] <- g[[i]] + ggtitle(names(tf)[i])
grid.arrange(g[[1]], g[[2]], g[[3]], nrow = 3)
```

We see the top 15 tokens for twitter, news, and blogs. Most of the top features in all the corpora 
are words that help construct sentences, like "the", "and", "to", "a". The inclusion of "you" and
"me" highlight how conversational twitter is. The news seems more objective, with "said", "he", and "was" all in the top 15. The blogs corpus appears to be a cross between the other
two; containing both "you" and "was".

Now we'll explore the *unique* number of uni-grams for each corpus:  
```{r unigrams}
print(nc <- sapply(dfm.en, ncol)) 
single <- sapply(dfm.en, function(x) sum(colSums(x) == 1))
```

While twitter may have the largest number here, it's important to remember what we're looking at. 
There hasn't been any filtering yet, only tokenization. That means everything with a space on both
sides is considered a uni-gram here. That includes numbers, @mentions, misspellings, etc. In fact,
of twitter's `r nc[1]` terms, only `r 100 - round((single/nc)[1] * 100, 0)`% are used more than once. This plot describes all the corpora:  

```{r}
mult <- data.frame("Single" = single, "Unique" = nc, 
                   "Percent.Single" = round((single/nc) * 100, 2))
#show size as amount of unique words, bar as %age used more than once
# or x and y axes as above. size = total tokens (unique and nonunique)
tot <- sapply(dfm.en, function(x) sum(ntoken(x)))
g <- ggplot(aes(x = Unique, y = Percent.Single), data = mult)
g + geom_point(aes(size = tot + 5e6), color = "grey")  + 
  geom_point(aes(size = tot ), color = "lightblue") +
  scale_size_continuous(range=c(10, 20 ), guide = FALSE) +
  labs(x = "Unique tokens", y = "Percentage used once", 
       title = "Unique Tokens vs Percentage Used Once, sized by Total Tokens") +
  geom_text(label=rownames(mult), color = "black", size = 8)

# + label each point
```


While 

##Future Plans  

###Model Design  
To save space, we will not store all possible *n*-grams indexed by their *n-1*-grams. Instead, we 
only need to store the *n*-gram with the highest frequency of it's *n-1*-gram index. For instance, 
if we have a term frequency list as follows:  

Term    Frequency    index
------- ---------- --------
the box  .05        the  
the cat  .15        the  
the oven  .02       the  
the king  .01       the  

We will only store the highest frequency term for the given index, "the". In fact, we don't even
need the frequency at that point.  

Term     index
-------  --------
the box  the  


###Filtering  

Filtering, such as profanity and numbers, are done at the n-gram level, so we don't have sentences
with gaps


###Down the Line  
Plan to add a part of speech (POS) tagger for unseen n-grams. We'll create a small frequency list 
for each of the parts of speech (possibly trained on our data). Then instead of blindly returning 
the highest occuring token, we will return the highest occuring token given the POS.  

We will keep a small number of lines from each corpus as a test set. We can create lots of *n*-gram
samples from just a few lines to test our model.  

Don't use backoff only for grams that don't appear. IE Where is the n-1 gram more frequent? From 
that take the nth word for the model.  